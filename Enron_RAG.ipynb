{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HmeBXxrR87F"
      },
      "source": [
        "So im going to need to reduce the size of the enron emails to 500 mb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6CamPF2iRWjZ",
        "outputId": "ed6cb3e0-8052-4564-dc75-d475caec3199"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.1/60.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m420.1/420.1 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.4 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.17 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain_community tiktoken langchain-openai langchainhub langchain -qU langchain-google-genai pymongo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Enron/emails.csv')"
      ],
      "metadata": {
        "id": "N3OTCUlKDFdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "RXFt0W6_DJkr",
        "outputId": "cb409f9d-a144-4256-edae-8cba668a2306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  message\n",
              "0       Mon, 14 May 2001 16:39:00 -0700 (PDT)\\nFrom: p...\n",
              "1       Fri, 4 May 2001 13:51:00 -0700 (PDT)\\nFrom: ph...\n",
              "2       Wed, 18 Oct 2000 03:00:00 -0700 (PDT)\\nFrom: p...\n",
              "3       Mon, 23 Oct 2000 06:13:00 -0700 (PDT)\\nFrom: p...\n",
              "4       Thu, 31 Aug 2000 05:07:00 -0700 (PDT)\\nFrom: p...\n",
              "...                                                   ...\n",
              "517396  Wed, 28 Nov 2001 13:30:11 -0800 (PST)\\nFrom: j...\n",
              "517397  Wed, 28 Nov 2001 12:47:48 -0800 (PST)\\nFrom: j...\n",
              "517398  Wed, 28 Nov 2001 07:20:00 -0800 (PST)\\nFrom: j...\n",
              "517399  Tue, 27 Nov 2001 11:52:45 -0800 (PST)\\nFrom: j...\n",
              "517400  Mon, 26 Nov 2001 10:48:43 -0800 (PST)\\nFrom: j...\n",
              "\n",
              "[517401 rows x 1 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31f29279-3492-48ed-8227-a34d3652f2c2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Mon, 14 May 2001 16:39:00 -0700 (PDT)\\nFrom: p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Fri, 4 May 2001 13:51:00 -0700 (PDT)\\nFrom: ph...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Wed, 18 Oct 2000 03:00:00 -0700 (PDT)\\nFrom: p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Mon, 23 Oct 2000 06:13:00 -0700 (PDT)\\nFrom: p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thu, 31 Aug 2000 05:07:00 -0700 (PDT)\\nFrom: p...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517396</th>\n",
              "      <td>Wed, 28 Nov 2001 13:30:11 -0800 (PST)\\nFrom: j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517397</th>\n",
              "      <td>Wed, 28 Nov 2001 12:47:48 -0800 (PST)\\nFrom: j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517398</th>\n",
              "      <td>Wed, 28 Nov 2001 07:20:00 -0800 (PST)\\nFrom: j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517399</th>\n",
              "      <td>Tue, 27 Nov 2001 11:52:45 -0800 (PST)\\nFrom: j...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>517400</th>\n",
              "      <td>Mon, 26 Nov 2001 10:48:43 -0800 (PST)\\nFrom: j...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>517401 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31f29279-3492-48ed-8227-a34d3652f2c2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-31f29279-3492-48ed-8227-a34d3652f2c2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-31f29279-3492-48ed-8227-a34d3652f2c2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-1527cb6a-592e-4d10-9586-8e3e78fc2c86\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-1527cb6a-592e-4d10-9586-8e3e78fc2c86')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-1527cb6a-592e-4d10-9586-8e3e78fc2c86 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_dc0b5342-5363-4a3f-9fd1-f9bfb0f212b5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('data')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dc0b5342-5363-4a3f-9fd1-f9bfb0f212b5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('data');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "data"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install cuml-cuda11 --extra-index-url=https://rapidsai.github.io/rapidsai-csp/colab  # Adjust based on your CUDA version\n",
        "# !pip install joblib pandas\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from cuml.manifold import UMAP\n",
        "\n",
        "# Load your embedding dataset from a CSV file.\n",
        "# The dataset should have 384 columns of embedding values.\n",
        "input_csv_path = \"/content/drive/MyDrive/Enron/embeddings.csv\"  # update path accordingly\n",
        "df = pd.read_csv(input_csv_path)\n",
        "\n",
        "# Convert DataFrame to a NumPy array and ensure it's float32 (for GPU compatibility)\n",
        "X = df.values.astype(np.float32)\n",
        "\n",
        "# Initialize UMAP for GPU with target dimensionality of 80.\n",
        "reducer = UMAP(n_components=80, random_state=42)\n",
        "\n",
        "# Fit UMAP on your data and transform it\n",
        "X_reduced = reducer.fit_transform(X)\n",
        "\n",
        "# Save the reduced embeddings to a new CSV (optional)\n",
        "output_csv_path = \"/content/drive/MyDrive/reduced_embeddings.csv\"\n",
        "pd.DataFrame(X_reduced, columns=[f\"umap_{i+1}\" for i in range(80)]).to_csv(output_csv_path, index=False)\n",
        "print(f\"Reduced embeddings saved to {output_csv_path}\")\n",
        "\n",
        "# Save the fitted UMAP model using joblib\n",
        "joblib_file = \"/content/drive/MyDrive/Enron/umap_model.joblib\"\n",
        "joblib.dump(reducer, joblib_file)\n",
        "print(f\"Fitted UMAP model saved to {joblib_file}\")\n"
      ],
      "metadata": {
        "id": "J4a_A735rfnQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef63448-4784-4dd8-fb0d-e616b6340e00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2025-03-28 17:49:38.977] [CUML] [info] build_algo set to brute_force_knn because random_state is given\n",
            "Reduced embeddings saved to /content/drive/MyDrive/reduced_embeddings.csv\n",
            "Fitted UMAP model saved to /content/drive/MyDrive/Enron/umap_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "si1_XCI1wL7L"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from cuml.manifold import UMAP\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
        "from urllib.parse import quote_plus\n",
        "from pymongo import MongoClient\n",
        "import joblib"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xORlqOmmR6io"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/Enron/Emails/emails.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v19mSu4Awige"
      },
      "outputs": [],
      "source": [
        "def compute_embeddings(texts):\n",
        "    def mean_pooling(model_output, attention_mask):\n",
        "        token_embeddings = model_output.last_hidden_state\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "    if isinstance(texts, str):\n",
        "        texts = [texts]\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model_name = \"intfloat/e5-small-v2\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model = AutoModel.from_pretrained(model_name).to(device)\n",
        "    input_texts = [f\"query: {text}\" for text in texts]\n",
        "    inputs = tokenizer(input_texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    sentence_embeddings = mean_pooling(outputs, inputs['attention_mask'])\n",
        "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
        "    embeddings = sentence_embeddings.cpu().numpy()\n",
        "    if len(texts) == 1:\n",
        "        return embeddings[0]\n",
        "    else:\n",
        "        return embeddings\n",
        "\n",
        "def reduce_embedding(embedding_array, target_dimensions=80):\n",
        "    if not isinstance(embedding_array, np.ndarray):\n",
        "        raise ValueError(\"Input should be a NumPy array.\")\n",
        "    if embedding_array.ndim == 1:\n",
        "        embedding_array = embedding_array.astype(np.float32).reshape(1, -1)\n",
        "    else:\n",
        "        embedding_array = embedding_array.astype(np.float32)\n",
        "    reducer = joblib.load('/content/drive/MyDrive/Enron/umap_model.joblib')\n",
        "    reduced_embeddings = reducer.transform(embedding_array)\n",
        "    return reduced_embeddings\n",
        "\n",
        "def generate_alternative_questions(question: str) -> list:\n",
        "    template = (\n",
        "        \"You are an AI language model assistant. Your task is to generate five different \"\n",
        "        \"versions of the given user question. Provide these alternative questions separated by newlines. \"\n",
        "        \"Original question: {question}\"\n",
        "    )\n",
        "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "    gemini_llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        google_api_key=\"API_KEY\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "    )\n",
        "    generate_queries = (\n",
        "        prompt_perspectives\n",
        "        | gemini_llm\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: [item for item in x.split(\"\\n\") if item.strip()])\n",
        "    )\n",
        "    result = generate_queries.invoke({\"question\": question})\n",
        "    return result\n",
        "\n",
        "def generate_sub_questions(question: str) -> list:\n",
        "    template = (\n",
        "        \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
        "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
        "Generate multiple search queries related to: {question} \\n\n",
        "Output (5 queries):\"\"\"\n",
        "    )\n",
        "    prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
        "    gemini_llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        google_api_key=\"API_KEY\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "    )\n",
        "    generate_queries = (\n",
        "        prompt_perspectives\n",
        "        | gemini_llm\n",
        "        | StrOutputParser()\n",
        "        | (lambda x: [item for item in x.split(\"\\n\") if item.strip()])\n",
        "    )\n",
        "    result = generate_queries.invoke({\"question\": question})\n",
        "    return result\n",
        "\n",
        "def generate_step_back_question(question: str) -> str:\n",
        "    examples = [\n",
        "        {\n",
        "            \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
        "            \"output\": \"what can the members of The Police do?\",\n",
        "        },\n",
        "        {\n",
        "            \"input\": \"Jan Sindel’s was born in what country?\",\n",
        "            \"output\": \"what is Jan Sindel’s personal history?\",\n",
        "        },\n",
        "    ]\n",
        "    example_prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"human\", \"{input}\"),\n",
        "        (\"ai\", \"{output}\")\n",
        "    ])\n",
        "    few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
        "        example_prompt=example_prompt,\n",
        "        examples=examples,\n",
        "    )\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"),\n",
        "        few_shot_prompt,\n",
        "        (\"user\", \"{question}\")\n",
        "    ])\n",
        "    gemini_llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        google_api_key=\"API_KEY\",\n",
        "        temperature=0,\n",
        "        max_tokens=None,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "    )\n",
        "    generate_query_chain = prompt | gemini_llm | StrOutputParser()\n",
        "    result = generate_query_chain.invoke({\"question\": question})\n",
        "    return result\n",
        "\n",
        "def vector_search(query_vector) -> list:\n",
        "    username = \"username\"\n",
        "    password = \"pass@123\"\n",
        "    database_name = \"dbname\"\n",
        "    collection_name = \"colname\"\n",
        "    encoded_username = quote_plus(username)\n",
        "    encoded_password = quote_plus(password)\n",
        "    mongo_uri = (\n",
        "        f\"mongodb+srv://{encoded_username}:{encoded_password}\"\n",
        "        \"@enroncluster.gmdslts.mongodb.net/?retryWrites=true&w=majority&appName=enroncluster\"\n",
        "    )\n",
        "    os.environ['MONGO_URI'] = mongo_uri\n",
        "    mongo_uri_env = os.environ.get('MONGO_URI')\n",
        "    client = MongoClient(mongo_uri_env)\n",
        "    db = client[database_name]\n",
        "    collection = db[collection_name]\n",
        "    if query_vector.ndim == 1:\n",
        "        query_vector = query_vector.reshape(1, -1)\n",
        "    nested_results = []\n",
        "    for vec in query_vector:\n",
        "        pipeline = [\n",
        "            {\n",
        "                \"$vectorSearch\": {\n",
        "                    \"queryVector\": vec.tolist(),\n",
        "                    \"path\": \"embedding\",\n",
        "                    \"numCandidates\": 100,\n",
        "                    \"limit\": 4,\n",
        "                    \"index\": \"enronsearch\"\n",
        "                }\n",
        "            },\n",
        "            {\n",
        "                \"$project\": {\"id\": 1, \"_id\": 0}\n",
        "            }\n",
        "        ]\n",
        "        results_cursor = collection.aggregate(pipeline)\n",
        "        results = list(results_cursor)\n",
        "        nested_results.append(results)\n",
        "    client.close()\n",
        "    return nested_results\n",
        "\n",
        "def get_emails(results):\n",
        "    df = pd.read_csv('/content/drive/MyDrive/Enron/emails.csv')\n",
        "    all_emails = []\n",
        "    for sublist in results:\n",
        "        line_emails = []\n",
        "        for res in sublist:\n",
        "            row_idx = res.get('id')\n",
        "            try:\n",
        "                email_val = df.iloc[row_idx]['message']\n",
        "            except IndexError:\n",
        "                email_val = None\n",
        "            line_emails.append(email_val)\n",
        "        all_emails.append(line_emails)\n",
        "    return all_emails\n",
        "\n",
        "def ask_question_with_context(question: str, nested_emails: list) -> str:\n",
        "    flattened_emails = [email for sublist in nested_emails for email in sublist if email]\n",
        "    context = \"\\n\\n\".join(flattened_emails)\n",
        "    template = \"\"\"Answer the following question based on this context:\n",
        "\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "    prompt = ChatPromptTemplate.from_template(template)\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-1.5-pro\",\n",
        "        google_api_key=\"API_KEY\",\n",
        "        temperature=0,\n",
        "        max_tokens=1000,\n",
        "        timeout=None,\n",
        "        max_retries=2,\n",
        "    )\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    output = chain.invoke({\"context\": context, \"question\": question})\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"when was enron in financial trouble\"\n",
        "step_back_question = generate_step_back_question(query)\n",
        "embedded_query = compute_embeddings(step_back_question)\n",
        "reduced_query = reduce_embedding(embedded_query)\n",
        "results = vector_search(reduced_query)\n",
        "emails = get_emails(results)\n",
        "answers = ask_question_with_context(query, emails)\n",
        "print(answers)"
      ],
      "metadata": {
        "id": "FQtXVFscCEqX"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}